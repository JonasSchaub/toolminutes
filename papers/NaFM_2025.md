---
author: Jonas Schaub
date: 2025-04-23
bibliography: ../references.bib
---

# NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products [@ding_nafm_2025]

## Why did I choose this paper?
- Was curious about what a "Foundation Model" in Natural Product Chemistry 
would be exactly, what could it be used for?

## Abstract
- Statement: existing deep learning methods for Natural Products (NP) research 
primarily rely on supervised learning approaches designed for specific 
downstream tasks
- Statement 2: one-model-for-a-task paradigm often lacks generalizability and 
leaves significant room for performance improvement
- Statement 3: existing models for chemical problems do not work well for NP
- This article describes NaFM ("Natural product Foundation Model"), a 
"pre-trained foundation model" for natural products
- Pre-training was done using masked graph learning and contrastive learning (see below)
- "Validation" of the approach was done in 3 experiments using NaFM as an encoder
and a Multi-Layer Perceptron (MLP) as a decoder to solve the following tasks:
  - NP classification
  - Predicting biological sources of NP
  - Predicting bioactivity of NP

## Introduction
- NP-based drug discovery/design with traditional methods (NP extracts from (cultivated) organisms, 
bioactivity-guided fractionation, target validation, structure elucidation, etc.) is costly and time-consuming
- With today's computing resources and databases like COCONUT, LOTUS, and Supernatural II,
NP drug discovery can be sped up, perspectively
- Prior work:
  - [@xu_composite_2024]: predicting kingdom of NP origin species
  - [@stokes_deep_2020]: predicting antibiotic activities
  - [@kim_npclassifier_2021]: predicting structural classes and source pathways of NP
  - Mostly use traditional fingerprints like Morgan/ECFP (Extended Connectivity FingerPrints)
  - "these methods could achieve satisfying performance
    but always suffer when applied to out-of-distribution test data or less-related downstream tasks."
  - They exclusively employ supervised learning but this is problematic because we do not
    have a lot of labelled data, and it is often biased (e.g. plant NP are usually overrepresented in
  NP databases)
- Pre-training on unlabelled data:
  - Historically, often done with SMILES as data, RNN or Transformers as model architectures, and 
  masked language modelling as a pre-training strategy
  - Lately, Graph Neural Networks (GNN) as model architectures, hence graphs as input data, and 
  pre-training strategies specifically for molecular graphs seem promising
- But no pre-trained foundation model for NP has been developed so far
- NP structures in general are more complex and diverse than synthetic compounds, 
have specific scaffolds and functional groups; which is why models trained on synthetic 
compounds usually struggle with NP
- But the advantage of NP structures is that they are "evolved", i.e. they often carry 
"preserved" substructures like scaffolds which depend on / relate to the source pathway or source organism
- And these scaffolds are related to the exhibited bioactivity
- A NP-specific pre-training strategy should therefore focus on scaffolds
- Contrastive learning, masked learning and acivity cliffs
  - Contrastive learning:
  - Masked learning:
  - Activity cliffs:

## Results

## Personal comments / questions to discuss
- There is no code available, apparently
- Larger text sections, e.g. the abstract, are quite hard to understand, i.e. there are many "nice" words but no details
- Is the presented model really a "foundation model" and not rather an "encoder"?

## References