---
author: Jonas Schaub
date: 2025-04-23
bibliography: ../references.bib
---

# NaFM: Pre-training a Foundation Model for Small-Molecule Natural Products [@ding_nafm_2025]

## Why did I choose this paper?
- Was curious about what a "Foundation Model" in Natural Product Chemistry 
would be exactly, what could it be used for?

## Abstract
- Statement: existing deep learning methods for Natural Products (NP) research 
primarily rely on supervised learning approaches designed for specific 
downstream tasks
- Statement 2: one-model-for-a-task paradigm often lacks generalizability and 
leaves significant room for performance improvement
- Statement 3: existing models for chemical problems do not work well for NP
- This article describes NaFM ("Natural product Foundation Model"), a 
"pre-trained foundation model" for natural products
- Pre-training was done using masked graph learning and contrastive learning (see below)
- "Validation" of the approach was done in 3 experiments using NaFM as a (kind-of) encoder
and a Multi-Layer Perceptron (MLP) as a (kind-of) decoder to solve the following tasks:
  - NP taxonomy classification
  - Predicting biological sources of NP
  - Predicting bioactivity of NP

## Introduction
- NP-based drug discovery/design with traditional methods (NP extracts from (cultivated) organisms, 
bioactivity-guided fractionation, target validation, structure elucidation, etc.) is costly and time-consuming
- With today's computing resources and databases like COCONUT, LOTUS, and Supernatural II,
NP drug discovery can be sped up, perspectively
- Prior work:
  - [@xu_composite_2024]: predicting kingdom of NP origin species
  - [@stokes_deep_2020]: predicting antibiotic activities
  - [@kim_npclassifier_2021]: predicting structural classes and source pathways of NP
  - Mostly use traditional fingerprints like Morgan/ECFP (Extended Connectivity FingerPrints)
  - "these methods could achieve satisfying performance
    but always suffer when applied to out-of-distribution test data or less-related downstream tasks."
  - They exclusively employ supervised learning but this is problematic because we do not
    have a lot of labelled data, and it is often biased (e.g. plant NP are usually overrepresented in
  NP databases)
- Pre-training on unlabelled data:
  - Historically, often done with SMILES as data, RNN or Transformers as model architectures, and 
  masked language modelling as a pre-training strategy
  - Lately, Graph Neural Networks (GNN) as model architectures, hence graphs as input data, and 
  pre-training strategies specifically for molecular graphs seem promising
- But no pre-trained foundation model for NP has been developed so far
- Some thoughts on NP structures in general:
  - NP structures in general are more complex and diverse than synthetic compounds, 
  have specific scaffolds (= ring systems) and functional groups; which is why models trained on synthetic 
  compounds usually struggle with NP
  - But the advantage of NP structures is that they are "evolved", i.e. they often carry 
  "preserved" substructures like scaffolds which depend on / relate to the source pathway or source organism
  - And these scaffolds are related to the exhibited bioactivity
  - A NP-specific pre-training strategy should therefore focus on scaffolds

\
![Figure 1 D. The "central dogma" of natural products. The biological source, biosynthetic gene clusters,
biosynthetic pathways, and bioactivity of natural products are interconnected through the scaffold, which acts as a bridge linking
these three key aspects.  source: [@ding_nafm_2025] CC-BY 4.0](NaFM_2025_images/Fig1d.png){width="600" fig-align="center"}
\

- Contrastive learning, masked learning and activity cliffs
  - Contrastive learning: in the learned embeddings, similar structures should have similar embeddings,
  and dissimilar structures should have dissimilar embeddings 
  - Masked learning: substructures are "masked", i.e. bond or atom attributes are omitted and have to be predicted by 
  the model based on the context
  - Existing approaches use the masking of structures simply as data augmentation 
  and define similarity/dissimilarity for contrastive learning with pairs of the original structures
  - Activity cliffs: very similar structures can have completely different (bio-)activities
  - Activity cliffs contradict the principles of contrastive learning
  - Idea pursued to avoid this here: use the masked structures as "similar" 
  structures to their original, unmasked structures
  - Also: mask whole substructures, not just single atoms and bonds, to make pre-training more 
  complex and hence more successful

## Model Architecture and Pre-Training
- NaFM: a pre-training framework ("pre-trained foundation model"?) specific to NP
- Two novel pre-training approaches: scaffold-subgraph reconstruction and scaffold-aware contrastive learning.

\
![Figure 1 B. Details of scaffold-subgraph reconstruction. First, a subgraph is randomly selected from the scaffold, consisting
of multiple atoms and chemical bonds. In the subgraph, both node and edge attributes are masked, and all nodes within the
subgraph are fully connected (i.e., expanded into a fully connected graph), thereby masking the topological information. During the
reconstruction process, the model needs to predict both node and edge attributes, while also distinguishing between real edges and
those artificially added virtual edges.  source: [@ding_nafm_2025] CC-BY 4.0](NaFM_2025_images/Fig1b.png){width="600" fig-align="center"}
\

- scaffold-subgraph reconstruction:
  - (not really evident in the figure, but) masking employed only to substructures of the scaffold of an NP!
  - atom information (elements / atom types, formal charge, chirality) is masked
  - bond information (multiplicity, direction (stereochemistry)) is masked
  - topological information is masked, all bonds in the substructure are removed and instead, 
the atoms become fully connected

\
![Figure 1 C. Details of scaffold-aware contrastive learning. Different colors represent varying weights
defined by scaffold similarity.  source: [@ding_nafm_2025] CC-BY 4.0](NaFM_2025_images/Fig1c.png){width="600" fig-align="center"}
\

- scaffold-aware contrastive learning:
  - For every original input structure, its masked form is set as positive example of a "similar" structure
that should be encoded similarly
  - the other molecules and their masked forms are set as negative examples, so "dissimilar" structures that should be
encoded dissimilarly
  - scaffold similarity is used as a metric for similarity of the complete structures
    - cosine similarity between the MACCS fingerprints of scaffolds is 
      incorporated into the contrastive learning loss function
  - This way, scaffold-subgraph reconstruction and scaffold-aware contrastive learning are combined

\
![Figure 1 A. Overview of NaFM pre-training. After the natural product molecule undergoes scaffold-aware masking, both the masked
and unmasked information are simultaneously input into a multi-layer Graph Isomorphism Network. The masked information is
then processed through a projection head to predict the masked atom attributes, bond attributes, and topological information.
Meanwhile, the masked and unmasked information passes through a pooling function and is used for scaffold-aware contrastive
learning.  source: [@ding_nafm_2025] CC-BY 4.0](NaFM_2025_images/Fig1a.png){width="600" fig-align="center"}
\

- Training data: COCONUT NP (0.6 mio molecules) ([@chandrasekhar_coconut_2024])
- Model architecture: Graph Neural Network (GNN) / Graph Isomorphism Network (GIN) (6 layers, hidden dimensionality of 512)
- Molecule goes in once as original structure and once as scaffold-masked structure
- Average pooling operation as the readout function to extract a global molecular representation of 1024 dimensions
- Projection head (3 Multi-Layer Perceptrons (MLP)) has to reconstruct/predict the masked properties (atom properties, bond properties, topologies)
- a MLP has to embed the original structures and their masked forms (in 256 dimensions)
so that original-masked pairs are seen as similar and other structures as dissimilar, based on scaffold 
similarity (scaffold-aware contrastive learning)

### General Down-stream Fine-tuning Approach
- Original contrastive learning head and reconstruction head are replaced with a randomly initialized MLP
- For all pre-trained baselines (other models that NaFM is compared to below) the same settings as described above are used 
- For baselines trained from scratch, their default configurations are used.

## NP Taxonomy Classification
- Data: NPClassifier dataset ([@kim_npclassifier_2021])
  - Biosynthetic pathway (7 types), super class (70), and class (563) annotations of 77,000 NP
- 

Table 1 The AUPRC results for natural product taxonomy classification compared to other baselines.

| Training samples per Class | 4            | 8            | 16           | 24           | 40           | 64           |
|----------------------------|--------------|--------------|--------------|--------------|--------------|--------------|
| N-Gram                     | 44.72 ± 1.91 | 56.61 ± 0.66 | 66.36 ± 1.57 | 71.11 ± 0.77 | 73.34 ± 1.14 | 75.77 ± 0.54 |
| PretrainGNN                | 44.83 ± 0.82 | 61.85 ± 0.78 | 75.76 ± 0.62 | 80.50 ± 0.17 | 85.31 ± 0.36 | 87.82 ± 0.13 |
| D-MPNN                     | 46.63 ± 0.23 | 60.88 ± 0.48 | 75.73 ± 0.12 | 80.96 ± 0.24 | 86.64 ± 0.44 | 89.23 ± 0.79 |
| MolCLR                     | 45.76 ± 1.98 | 65.80 ± 1.51 | 78.14 ± 1.08 | 83.20 ± 0.74 | 85.56 ± 0.11 | 88.22 ± 0.34 |
| Mole-BERT                  | 66.32 ± 1.23 | 73.39 ± 0.66 | 78.25 ± 0.45 | 80.83 ± 0.51 | 83.57 ± 0.59 | 85.69 ± 0.70 |
| ECFP                       | 69.17 ± 0.19 | 78.21 ± 0.79 | 83.82 ± 0.37 | 86.28 ± 0.49 | 88.52 ± 0.48 | 89.75 ± 0.45 |
| GraphMVP                   | 64.50 ± 0.74 | 78.41 ± 0.09 | 85.71 ± 0.43 | 87.88 ± 0.26 | 89.72 ± 0.45 | 91.07 ± 0.43 |
| **NaFM**                   | 70.10 ± 0.92 | 79.89 ± 0.07 | 87.37 ± 1.51 | 89.15 ± 0.22 | 90.77 ± 0.26 | 91.75 ± 0.47 |

\
![Figure 2. The bar plots illustrate the models’ performance comparison of NaFM and NPClassifier on various superclass categories
(a) and biosynthetic pathways (b) including Carbohydrates, Amino Acids and Peptides, Alkaloids, Terpenoids, Shikimates and
Phenylpropanoids, Polyketides, and Fatty Acids in terms of AUPRC. The gray segments indicate the shared performance (minimum
AUPRC) and the light blue/orange segments highlight the additional AURPC achieved by NaFM and NPClassifier beyond each
other, respectively. Higher values indicate better model performance.  source: [@ding_nafm_2025] CC-BY 4.0](NaFM_2025_images/Fig2.png){width="600" fig-align="center"}
\

## Biosynthesis Prediction
- Based on LOTUS ([@rutz_lotus_2022])
- 

\
![Figure 3. The obtained representations are projected into two dimensions using TMAP and
visualized with Faerun. For each biological source, a representative example is highlighted in a designated box, showcasing a
characteristic scaffold that typifies compounds derived from that particular source.  source: [@ding_nafm_2025] CC-BY 4.0](NaFM_2025_images/Fig3.png){width="600" fig-align="center"}
\

## Prediction of NP-producing Biosynthetic Gene Cluster (BGC) and protein
- ?

\
![Figure 4. The figure visualizes the AUROC gap between NaFM and various molecular representations. The results show the AUROC
for the 128 most frequent protein families found in bacteria and fungi. The color in each block represents the magnitude of the
AUROC gap, ranging from negative to positive, with colors transitioning from red to blue. Deeper colors indicate a greater absolute
difference.  source: [@ding_nafm_2025] CC-BY 4.0](NaFM_2025_images/Fig4.png){width="600" fig-align="center"}
\

## Predicting bioactivity for specific targets
- dscbs
- Figure 5?

## Conclusion
- sfbfbi

## Personal comments / questions to discuss
- There is no code available, apparently; sad
- Larger text sections, e.g. the abstract, are quite hard to understand, i.e. there are many "nice" words but no details
- Is the presented model really a "foundation model" and not rather an "encoder"?

## References